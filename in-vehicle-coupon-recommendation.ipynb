{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f1a43d",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170957bd",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "0d349a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/in-vehicle-coupon-recommendation.csv\")\n",
    "\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048538c3",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ef562",
   "metadata": {},
   "source": [
    "As a very first step we are going to drop 2 of the features from our dataset, one of them being the feature car, because our dataset includes values of this feature for only 109 records, and the second one will be direction_opp, because it's the complete opposite of direction_same and together they would be redundant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "68727995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['car','direction_opp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632002e4",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbfc8c",
   "metadata": {},
   "source": [
    "One of our features has mixed values. That is the feature age and the possible values are the following: \"21, 46, 26, 31, 41, 50plus, 36, below21\". For that reason we will try to engineer new columns based on this one, we will try 2 options: converting it to numeric values and categorical and we'll see which one does better for our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "06ae8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_age_categorical(value):\n",
    "    try:\n",
    "        value = int(value)\n",
    "        if value < 21:\n",
    "            return \"<21\"\n",
    "        elif value <= 30:\n",
    "            return \"21-30\"\n",
    "        elif value <= 40:\n",
    "            return \"31-40\"\n",
    "        elif value <= 50:\n",
    "            return \"41-50\"\n",
    "        else:\n",
    "            return \"51+\"\n",
    "    except:\n",
    "        if str(value).lower() == \"below21\":\n",
    "            return \"<21\"\n",
    "        elif str(value).lower() == \"50plus\":\n",
    "            return \"51+\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "\n",
    "def convert_age_numeric(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        mapping = {\n",
    "            \"below21\": 20,\n",
    "            \"50plus\": 55\n",
    "        }\n",
    "        return mapping.get(value.strip(), None)\n",
    "\n",
    "\n",
    "data['age_numeric'] = data['age'].apply(convert_age_numeric)\n",
    "data['age_group'] = data['age'].apply(convert_age_categorical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ec3da",
   "metadata": {},
   "source": [
    "We will try a similar engineering with income featue, which has the following values: \"$37500 - $49999, $62500 - $74999, $12500 - $24999, $75000 - $87499, $50000 - $62499, $25000 - $37499, $100000 or More, $87500 - $99999, Less than $12500\". \n",
    "As we can see there is obviously an ordered numeric meaning behind it, so we will try 2 way again: categorical and numeric.\n",
    "\n",
    "We are going to drop former age and income columns after engineering new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "afd3da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_income_numeric(value):\n",
    "    value = value.strip()\n",
    "\n",
    "    if value == \"Less than $12500\":\n",
    "        return 6250\n",
    "    elif value == \"$12500 - $24999\":\n",
    "        return (12500 + 24999) / 2\n",
    "    elif value == \"$25000 - $37499\":\n",
    "        return (25000 + 37499) / 2\n",
    "    elif value == \"$37500 - $49999\":\n",
    "        return (37500 + 49999) / 2\n",
    "    elif value == \"$50000 - $62499\":\n",
    "        return (50000 + 62499) / 2\n",
    "    elif value == \"$62500 - $74999\":\n",
    "        return (62500 + 74999) / 2\n",
    "    elif value == \"$75000 - $87499\":\n",
    "        return (75000 + 87499) / 2\n",
    "    elif value == \"$87500 - $99999\":\n",
    "        return (87500 + 99999) / 2\n",
    "    elif value == \"$100000 or More\":\n",
    "        return 110000\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def convert_income_categorical(value):\n",
    "    mapping = {\n",
    "        \"Less than $12500\": \"Under 12.5k\",\n",
    "        \"$12500 - $24999\": \"12.5k-25k\",\n",
    "        \"$25000 - $37499\": \"25k-37k\",\n",
    "        \"$37500 - $49999\": \"37k-49k\",\n",
    "        \"$50000 - $62499\": \"50k-62k\",\n",
    "        \"$62500 - $74999\": \"62k-74k\",\n",
    "        \"$75000 - $87499\": \"75k-87k\",\n",
    "        \"$87500 - $99999\": \"87k-99k\",\n",
    "        \"$100000 or More\": \"100k+\"\n",
    "    }\n",
    "    return mapping.get(value.strip(), None)\n",
    "\n",
    "data['income_numeric'] = data['income'].apply(convert_income_numeric)\n",
    "data['income_group'] = data['income'].apply(convert_income_categorical)\n",
    "data = data.drop(['age', 'income'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6ef5d",
   "metadata": {},
   "source": [
    "After doing these steps on the whole dataset, which is ok because these are just rule based modifications so there's no risk of data leakage, we will split the dataset into train and test sets and do our further modifications in a pipeline to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "520a0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 7\n",
    "X = data.drop('Y', axis=1)\n",
    "y = data['Y']\n",
    "\n",
    "# Split for final training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f62f4",
   "metadata": {},
   "source": [
    "### Cyclical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62dc546",
   "metadata": {},
   "source": [
    "For the time feature, we will use cyclical encoding because it has a circular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c0cb8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def custom_cyclical_encoder_func(X):\n",
    "    X = X.copy()\n",
    "\n",
    "    # Convert time (e.g., \"2PM\") to hour\n",
    "    X['hour'] = pd.to_datetime(X['time'], format='%I%p').dt.hour\n",
    "\n",
    "    # Add sine and cosine transformations\n",
    "    X['hour_sin'] = np.sin(2 * np.pi * X['hour'] / 24)\n",
    "    X['hour_cos'] = np.cos(2 * np.pi * X['hour'] / 24)\n",
    "\n",
    "    # Drop original columns\n",
    "    X.drop(columns=['time', 'hour'], inplace=True)\n",
    "\n",
    "    return X\n",
    "\n",
    "custom_cyclical_encoder = FunctionTransformer(custom_cyclical_encoder_func)\n",
    "\n",
    "# transform X_train for further skew checks\n",
    "X_train_transformed = custom_cyclical_encoder.transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8d9a8",
   "metadata": {},
   "source": [
    "### Bias, Skew, Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Skew\n",
    "print(\"Skew\")\n",
    "skew_features = ['temperature', 'age_numeric', 'income_numeric']\n",
    "print(X_train_transformed[skew_features].skew())\n",
    "\n",
    "\n",
    "# # Correlation\n",
    "print(\"Correlation\")\n",
    "corr_features = ['temperature', 'income_numeric', 'age_numeric', 'hour_sin', 'hour_cos']\n",
    "corr_matrix = X_train_transformed[corr_features].corr()\n",
    "\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c142585",
   "metadata": {},
   "source": [
    "Modifications on features based on skew and correlation. \n",
    "\n",
    "We have moderate negative (left skew) for temperature and moderate positive (right skew) for age_numeric, so what we'll do is use Box-Cox transformer, which automaticall handles both left and right skewness of the data, and works only for positive values, which is the case for both of our features. \n",
    "\n",
    "And based on the correlation matrix our features look good, no strong correlation between variables so we can keep all of them for now.\n",
    "\n",
    "\n",
    "We will also use scaling on our numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "3e4bffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "skewed_features = ['temperature', 'age_numeric']\n",
    "\n",
    "# only income_numeric, because the other 2 will already be scaled during box-cox transformation\n",
    "numeric_features_to_scale = ['income_numeric']\n",
    "\n",
    "box_cox_transformer = PowerTransformer(method='box-cox', standardize=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# this will be re-written at the end to contain the rest of the modifications for other features as well!!!\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('box_cox_transform', box_cox_transformer, skewed_features),\n",
    "    ('scale_only', scaler, numeric_features_to_scale),\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f221828",
   "metadata": {},
   "source": [
    "Now let's look at the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "bias_features = ['gender', 'maritalStatus', 'occupation', 'passanger', 'destination', 'coupon', 'weather', 'education', 'income_group', 'age_group', 'has_children', 'toCoupon_GEQ5min', 'toCoupon_GEQ15min', 'toCoupon_GEQ25min', 'direction_same', 'expiration', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
    " \n",
    "print(\"Bias\")\n",
    "for col in bias_features:\n",
    "    # print(data[col].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"\\n--- Value Distribution for '{col}' ---\")\n",
    "    print((X_train_transformed[col].value_counts(normalize=True) * 100).round(2).rename(\"percentage\").to_frame())\n",
    "    # print(data[col].value_counts(normalize=True).rename(\"proportion\").to_frame())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11784a",
   "metadata": {},
   "source": [
    "One of the features that is imbalanced is marital status. So what we'll do is join some of the groups into one for the less represented groups to have more statistical effect and to reduce the noise in the data, this will help to avoid overfitting in our models.\n",
    "\n",
    "We can say the same about education and occupation and a few other features. So we will do the same re-grouping for them. In case of occupation, though we also have high cardinality, so it will help with this issue as well. \n",
    "\n",
    "We can also see that 2 of our features, weather and direction_same, we also have very high bias. But it was decided to keep these variables as they are because in our opinion they may hold important information and we don't want to lose it at early stages. \n",
    "\n",
    "Lastly, we will drop toCoupon_GEQ25min, because it has 88% 0s in it, and toCoupon_GEQ5min, because all the values here are the same, these features will be redundant in our analysis, and after all we also have toCoupon_GEQ15min, which is balanced and can give us an idea whether the driving distance effects the decision making or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "2cb8ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_balancing_func(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    X.drop(['toCoupon_GEQ25min', 'toCoupon_GEQ5min'], axis=1, inplace=True)\n",
    "\n",
    "    # Mappings for regrouping\n",
    "    replace_maps = {\n",
    "        'maritalStatus': {\n",
    "            \"Divorced\": \"Previously Married\",\n",
    "            \"Widowed\": \"Previously Married\"\n",
    "        },\n",
    "        'education': {\n",
    "            \"Some High School\": \"High School or Less\",\n",
    "            \"High School Graduate\": \"High School or Less\",\n",
    "            \"Associates degree\": \"Some College\",\n",
    "            \"Some college - no degree\": \"Some College\"\n",
    "        },\n",
    "        'occupation': {\n",
    "            \"Architecture & Engineering\": \"Professional, Scientific & Technical\",\n",
    "            \"Computer & Mathematical\": \"Professional, Scientific & Technical\",\n",
    "            \"Legal\": \"Professional, Scientific & Technical\",\n",
    "            \"Healthcare Support\": \"Healthcare\",\n",
    "            \"Healthcare Practitioners & Technical\": \"Healthcare\",\n",
    "            \"Management\": \"Management & Business\",\n",
    "            \"Business & Financial\": \"Management & Business\",\n",
    "            \"Sales & Related\": \"Admin/Sales\",\n",
    "            \"Office & Administrative Support\": \"Admin/Sales\",\n",
    "            \"Education&Training&Library\": \"Education\",\n",
    "            \"Arts Design Entertainment Sports & Media\": \"Arts/Media\",\n",
    "            \"Life Physical Social Science\": \"Social Work and Service\", \n",
    "            \"Community & Social Services\": \"Social Work and Service\",\n",
    "            \"Personal Care & Service\": \"Social Work and Service\",\n",
    "            \"Food Preparation & Serving Related\": \"Social Work and Service\",\n",
    "            \"Protective Service\": \"Social Work and Service\",\n",
    "            \"Building & Grounds Cleaning & Maintenance\": \"Social Work and Service\",\n",
    "            \"Construction & Extraction\": \"Transportation/Manual\",\n",
    "            \"Installation Maintenance & Repair\": \"Transportation/Manual\",\n",
    "            \"Transportation & Material Moving\": \"Transportation/Manual\",\n",
    "            \"Production Occupations\": \"Transportation/Manual\",\n",
    "            \"Farming Fishing & Forestry\": \"Transportation/Manual\",\n",
    "            \"Unemployed\": \"Student/Unemployed\",\n",
    "            \"Student\": \"Student/Unemployed\",\n",
    "            \"Retired\": \"Student/Unemployed\"\n",
    "        },\n",
    "        'passanger': {\n",
    "            \"Kid(s)\": \"Kid(s) or Partner\",\n",
    "            \"Partner\": \"Kid(s) or Partner\"\n",
    "        },\n",
    "        'CarryAway': {\n",
    "            \"never\": \"less1\"\n",
    "        },\n",
    "        'RestaurantLessThan20': {\n",
    "            \"never\": \"less1\"\n",
    "        },\n",
    "        'Bar': {\n",
    "            \"4~8\": \"4+\",\n",
    "            \"gt8\": \"4+\"\n",
    "        },\n",
    "        'Restaurant20To50': {\n",
    "            \"4~8\": \"4+\",\n",
    "            \"gt8\": \"4+\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Apply all mappings\n",
    "    for col, mapping in replace_maps.items():\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].replace(mapping)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "custom_balancing_function = FunctionTransformer(custom_balancing_func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01179c7",
   "metadata": {},
   "source": [
    "### Encoding ( One-hot and Ordinal )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62a158",
   "metadata": {},
   "source": [
    "For categorical data like destination, passanger, weather, coupon, gender, marital status, occupation we will use one-hot encoding, which is good for nominal data, when there is no order/ranking between the categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "d022385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "nominal_cat_features = ['destination', 'passanger', 'weather', 'coupon', 'gender', 'maritalStatus', 'occupation']\n",
    "\n",
    "one_hot_encoder = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86d31e",
   "metadata": {},
   "source": [
    "For the rest of the categorical data we will use a different method of encoding because these are considered ordinal data, where categories have order/ranking. Some of these features also have missing values, so we will replace them with the most frequent value of the category.\n",
    "\n",
    "We also give the order of the categories for the processor to know the correct order and give correct importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "c1c557dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_cat_features = ['expiration', 'education', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50', 'age_group', 'income_group']\n",
    "\n",
    "ordinal_categories = [\n",
    "    ['2h', '1d'],   # expiration\n",
    "    ['High School or Less', 'Some College', 'Bachelors degree', 'Graduate degree (Masters or Doctorate)'], # education\n",
    "    ['never', 'less1', '1~3', '4+'],    # Bar\n",
    "    ['never', 'less1', '1~3', '4~8', 'gt8'],    # CoffeeHouse\n",
    "    ['less1', '1~3', '4~8', 'gt8'],    # CarryAway\n",
    "    ['less1', '1~3', '4~8', 'gt8'],    # RestaurantLessThan20\n",
    "    ['never', 'less1', '1~3', '4+'],     # Restaurant20To50\n",
    "    ['<21', '21-30', '31-40', '41-50', '51+'], # age_group\n",
    "    ['Under 12.5k', '12.5k-25k', '25k-37k', '37k-49k','50k-62k','62k-74k','75k-87k','87k-99k','100k+'] # income_group\n",
    "]\n",
    "\n",
    "ordinal_encoder = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc1a74",
   "metadata": {},
   "source": [
    "### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we'll do the split again from 0, so that our former modifactions don't effect the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# \n",
    "\n",
    "# transformers\n",
    "custom_cyclical_encoder = FunctionTransformer(custom_cyclical_encoder_func)\n",
    "custom_balancing_function = FunctionTransformer(custom_balancing_func)\n",
    "box_cox_transformer = PowerTransformer(method='box-cox', standardize=True)\n",
    "scaler = StandardScaler()\n",
    "one_hot_encoder = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "ordinal_encoder = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "])\n",
    "\n",
    "\n",
    "# columns\n",
    "skewed_features = ['temperature', 'age_numeric']\n",
    "numeric_features_to_scale = ['income_numeric']\n",
    "nominal_cat_features = ['destination', 'passanger', 'weather', 'coupon', 'gender', 'maritalStatus', 'occupation']\n",
    "ordinal_cat_features = ['expiration', 'education', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50', 'age_group', 'income_group']\n",
    "\n",
    "\n",
    "column_preprocessor = ColumnTransformer(transformers = [\n",
    "    ('box_cox_transform', box_cox_transformer, skewed_features),\n",
    "    ('num_scale', scaler, numeric_features_to_scale),\n",
    "    ('nom', one_hot_encoder, nominal_cat_features),\n",
    "    ('ord', ordinal_encoder, ordinal_cat_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('cyclical', custom_cyclical_encoder),\n",
    "    ('balancing', custom_balancing_function),\n",
    "    ('column_transforms', column_preprocessor)\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "column_names = preprocessor.named_steps['column_transforms'].get_feature_names_out()\n",
    "X_processed = preprocessor.transform(X_train)\n",
    "\n",
    "transformed_df = pd.DataFrame(X_processed, columns=column_names)\n",
    "# print(transformed_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
