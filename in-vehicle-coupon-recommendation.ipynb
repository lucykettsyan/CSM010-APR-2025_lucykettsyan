{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f1a43d",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170957bd",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "0d349a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/in-vehicle-coupon-recommendation.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048538c3",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ef562",
   "metadata": {},
   "source": [
    "As a very first step we are going to drop 2 of the features from our dataset, one of them being the feature car, because our dataset includes values of this feature for only 109 records, and the second one will be direction_opp, because it's the complete opposite of direction_same and together they would be redundant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "68727995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['car','direction_opp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632002e4",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbfc8c",
   "metadata": {},
   "source": [
    "One of our features has mixed values. That is the feature age and the possible values are the following: \"21, 46, 26, 31, 41, 50plus, 36, below21\". For that reason we will try to engineer new columns based on this one, we will try 2 options: converting it to numeric values and categorical and we'll see which one does better for our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "06ae8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_age_categorical(value):\n",
    "    try:\n",
    "        value = int(value)\n",
    "        if value < 21:\n",
    "            return \"<21\"\n",
    "        elif value <= 30:\n",
    "            return \"21-30\"\n",
    "        elif value <= 40:\n",
    "            return \"31-40\"\n",
    "        elif value <= 50:\n",
    "            return \"41-50\"\n",
    "        else:\n",
    "            return \"51+\"\n",
    "    except:\n",
    "        if str(value).lower() == \"below21\":\n",
    "            return \"<21\"\n",
    "        elif str(value).lower() == \"50plus\":\n",
    "            return \"51+\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "\n",
    "def convert_age_numeric(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        mapping = {\n",
    "            \"below21\": 20,\n",
    "            \"50plus\": 55\n",
    "        }\n",
    "        return mapping.get(value.strip(), None)\n",
    "\n",
    "\n",
    "data['age_numeric'] = data['age'].apply(convert_age_numeric)\n",
    "data['age_group'] = data['age'].apply(convert_age_categorical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ec3da",
   "metadata": {},
   "source": [
    "We will try a similar engineering with income featue, which has the following values: \"$37500 - $49999, $62500 - $74999, $12500 - $24999, $75000 - $87499, $50000 - $62499, $25000 - $37499, $100000 or More, $87500 - $99999, Less than $12500\". \n",
    "As we can see there is obviously an ordered numeric meaning behind it, so we will try 2 way again: categorical and numeric.\n",
    "\n",
    "We are going to drop former age and income columns after engineering new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "afd3da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_income_numeric(value):\n",
    "    value = value.strip()\n",
    "\n",
    "    if value == \"Less than $12500\":\n",
    "        return 6250\n",
    "    elif value == \"$12500 - $24999\":\n",
    "        return (12500 + 24999) / 2\n",
    "    elif value == \"$25000 - $37499\":\n",
    "        return (25000 + 37499) / 2\n",
    "    elif value == \"$37500 - $49999\":\n",
    "        return (37500 + 49999) / 2\n",
    "    elif value == \"$50000 - $62499\":\n",
    "        return (50000 + 62499) / 2\n",
    "    elif value == \"$62500 - $74999\":\n",
    "        return (62500 + 74999) / 2\n",
    "    elif value == \"$75000 - $87499\":\n",
    "        return (75000 + 87499) / 2\n",
    "    elif value == \"$87500 - $99999\":\n",
    "        return (87500 + 99999) / 2\n",
    "    elif value == \"$100000 or More\":\n",
    "        return 110000\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def convert_income_categorical(value):\n",
    "    mapping = {\n",
    "        \"Less than $12500\": \"Under 12.5k\",\n",
    "        \"$12500 - $24999\": \"12.5k-25k\",\n",
    "        \"$25000 - $37499\": \"25k-37k\",\n",
    "        \"$37500 - $49999\": \"37k-49k\",\n",
    "        \"$50000 - $62499\": \"50k-62k\",\n",
    "        \"$62500 - $74999\": \"62k-74k\",\n",
    "        \"$75000 - $87499\": \"75k-87k\",\n",
    "        \"$87500 - $99999\": \"87k-99k\",\n",
    "        \"$100000 or More\": \"100k+\"\n",
    "    }\n",
    "    return mapping.get(value.strip(), None)\n",
    "\n",
    "data['income_numeric'] = data['income'].apply(convert_income_numeric)\n",
    "data['income_group'] = data['income'].apply(convert_income_categorical)\n",
    "data = data.drop(['age', 'income'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6ef5d",
   "metadata": {},
   "source": [
    "After doing these steps on the whole dataset, which is ok because these are just rule based modifications so there's no risk of data leakage, we will split the dataset into train and test sets and do our further modifications in a pipeline to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "520a0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 7\n",
    "X = data.drop('Y', axis=1)\n",
    "y = data['Y']\n",
    "\n",
    "# Split for final training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f62f4",
   "metadata": {},
   "source": [
    "### Cyclical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62dc546",
   "metadata": {},
   "source": [
    "For the time feature, we will use cyclical encoding because it has a circular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "c0cb8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def custom_cyclical_encoder_func(X):\n",
    "    X = X.copy()\n",
    "\n",
    "    # Convert time (e.g., \"2PM\") to hour\n",
    "    X['hour'] = pd.to_datetime(X['time'], format='%I%p').dt.hour\n",
    "\n",
    "    # Add sine and cosine transformations\n",
    "    X['hour_sin'] = np.sin(2 * np.pi * X['hour'] / 24)\n",
    "    X['hour_cos'] = np.cos(2 * np.pi * X['hour'] / 24)\n",
    "\n",
    "    # Drop original columns\n",
    "    X.drop(columns=['time', 'hour'], inplace=True)\n",
    "\n",
    "    return X\n",
    "\n",
    "custom_cyclical_encoder = FunctionTransformer(custom_cyclical_encoder_func)\n",
    "\n",
    "# transform X_train for further skew checks\n",
    "X_train_transformed = custom_cyclical_encoder.transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8d9a8",
   "metadata": {},
   "source": [
    "### Bias, Skew, Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Skew\n",
    "print(\"Skew\")\n",
    "skew_features = ['temperature', 'age_numeric', 'income_numeric']\n",
    "print(X_train_transformed[skew_features].skew())\n",
    "\n",
    "\n",
    "# # Correlation\n",
    "print(\"Correlation\")\n",
    "corr_features = ['temperature', 'income_numeric', 'age_numeric', 'hour_sin', 'hour_cos']\n",
    "corr_matrix = X_train_transformed[corr_features].corr()\n",
    "\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c142585",
   "metadata": {},
   "source": [
    "Modifications on features based on skew and correlation. \n",
    "\n",
    "We have moderate negative (left skew) for temperature and moderate positive (right skew) for age_numeric, so what we'll do is use Box-Cox transformer, which automaticall handles both left and right skewness of the data, and works only for positive values, which is the case for both of our features. \n",
    "\n",
    "And based on the correlation matrix our features look good, no strong correlation between variables so we can keep all of them for now.\n",
    "\n",
    "\n",
    "We will also use scaling on our numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "3e4bffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "skewed_features = ['temperature', 'age_numeric']\n",
    "\n",
    "# only income_numeric, because the other 2 will already be scaled during box-cox transformation\n",
    "numeric_features_to_scale = ['income_numeric']\n",
    "\n",
    "box_cox_transformer = PowerTransformer(method='box-cox', standardize=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# this will be re-written at the end to contain the rest of the modifications for other features as well!!!\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('box_cox_transform', box_cox_transformer, skewed_features),\n",
    "    ('scale_only', scaler, numeric_features_to_scale),\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f221828",
   "metadata": {},
   "source": [
    "Now let's look at the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "bias_features = ['gender', 'maritalStatus', 'occupation', 'passanger', 'destination', 'coupon', 'weather', 'education', 'income_group', 'age_group', 'has_children', 'toCoupon_GEQ5min', 'toCoupon_GEQ15min', 'toCoupon_GEQ25min', 'direction_same', 'expiration', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
    " \n",
    "print(\"Bias\")\n",
    "for col in bias_features:\n",
    "    # print(data[col].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"\\n--- Value Distribution for '{col}' ---\")\n",
    "    print((X_train_transformed[col].value_counts(normalize=True) * 100).round(2).rename(\"percentage\").to_frame())\n",
    "    # print(data[col].value_counts(normalize=True).rename(\"proportion\").to_frame())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11784a",
   "metadata": {},
   "source": [
    "One of the features that is imbalanced is marital status. So what we'll do is join some of the groups into one for the less represented groups to have more statistical effect and to reduce the noise in the data, this will help to avoid overfitting in our models.\n",
    "\n",
    "We can say the same about education and occupation and a few other features. So we will do the same re-grouping for them. In case of occupation, though we also have high cardinality, so it will help with this issue as well. \n",
    "\n",
    "We can also see that 2 of our features, weather and direction_same, we also have very high bias. But it was decided to keep these variables as they are because in our opinion they may hold important information and we don't want to lose it at early stages. \n",
    "\n",
    "Lastly, we will drop toCoupon_GEQ25min, because it has 88% 0s in it, and toCoupon_GEQ5min, because all the values here are the same, these features will be redundant in our analysis, and after all we also have toCoupon_GEQ15min, which is balanced and can give us an idea whether the driving distance effects the decision making or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "2cb8ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_balancing_func(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    X.drop(['toCoupon_GEQ25min', 'toCoupon_GEQ5min'], axis=1, inplace=True)\n",
    "\n",
    "    # Mappings for regrouping\n",
    "    replace_maps = {\n",
    "        'maritalStatus': {\n",
    "            \"Divorced\": \"Previously Married\",\n",
    "            \"Widowed\": \"Previously Married\"\n",
    "        },\n",
    "        'education': {\n",
    "            \"Some High School\": \"High School or Less\",\n",
    "            \"High School Graduate\": \"High School or Less\",\n",
    "            \"Associates degree\": \"Some College\",\n",
    "            \"Some college - no degree\": \"Some College\"\n",
    "        },\n",
    "        'occupation': {\n",
    "            \"Architecture & Engineering\": \"Professional, Scientific & Technical\",\n",
    "            \"Computer & Mathematical\": \"Professional, Scientific & Technical\",\n",
    "            \"Legal\": \"Professional, Scientific & Technical\",\n",
    "            \"Healthcare Support\": \"Healthcare\",\n",
    "            \"Healthcare Practitioners & Technical\": \"Healthcare\",\n",
    "            \"Management\": \"Management & Business\",\n",
    "            \"Business & Financial\": \"Management & Business\",\n",
    "            \"Sales & Related\": \"Admin/Sales\",\n",
    "            \"Office & Administrative Support\": \"Admin/Sales\",\n",
    "            \"Education&Training&Library\": \"Education\",\n",
    "            \"Arts Design Entertainment Sports & Media\": \"Arts/Media\",\n",
    "            \"Life Physical Social Science\": \"Social Work and Service\", \n",
    "            \"Community & Social Services\": \"Social Work and Service\",\n",
    "            \"Personal Care & Service\": \"Social Work and Service\",\n",
    "            \"Food Preparation & Serving Related\": \"Social Work and Service\",\n",
    "            \"Protective Service\": \"Social Work and Service\",\n",
    "            \"Building & Grounds Cleaning & Maintenance\": \"Social Work and Service\",\n",
    "            \"Construction & Extraction\": \"Transportation/Manual\",\n",
    "            \"Installation Maintenance & Repair\": \"Transportation/Manual\",\n",
    "            \"Transportation & Material Moving\": \"Transportation/Manual\",\n",
    "            \"Production Occupations\": \"Transportation/Manual\",\n",
    "            \"Farming Fishing & Forestry\": \"Transportation/Manual\",\n",
    "            \"Unemployed\": \"Student/Unemployed\",\n",
    "            \"Student\": \"Student/Unemployed\",\n",
    "            \"Retired\": \"Student/Unemployed\"\n",
    "        },\n",
    "        'passanger': {\n",
    "            \"Kid(s)\": \"Kid(s) or Partner\",\n",
    "            \"Partner\": \"Kid(s) or Partner\"\n",
    "        },\n",
    "        'CarryAway': {\n",
    "            \"never\": \"less1\"\n",
    "        },\n",
    "        'RestaurantLessThan20': {\n",
    "            \"never\": \"less1\"\n",
    "        },\n",
    "        'Bar': {\n",
    "            \"4~8\": \"4+\",\n",
    "            \"gt8\": \"4+\"\n",
    "        },\n",
    "        'Restaurant20To50': {\n",
    "            \"4~8\": \"4+\",\n",
    "            \"gt8\": \"4+\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Apply all mappings\n",
    "    for col, mapping in replace_maps.items():\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].replace(mapping)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "custom_balancing_function = FunctionTransformer(custom_balancing_func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01179c7",
   "metadata": {},
   "source": [
    "### Encoding ( One-hot and Ordinal )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62a158",
   "metadata": {},
   "source": [
    "For categorical data like destination, passanger, weather, coupon, gender, marital status, occupation we will use one-hot encoding, which is good for nominal data, when there is no order/ranking between the categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "d022385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "nominal_cat_features = ['destination', 'passanger', 'weather', 'coupon', 'gender', 'maritalStatus', 'occupation']\n",
    "\n",
    "one_hot_encoder = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86d31e",
   "metadata": {},
   "source": [
    "For the rest of the categorical data we will use a different method of encoding because these are considered ordinal data, where categories have order/ranking. Some of these features also have missing values, so we will replace them with the most frequent value of the category.\n",
    "\n",
    "We also give the order of the categories for the processor to know the correct order and give correct importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "c1c557dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_cat_features = ['expiration', 'education', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50', 'age_group', 'income_group']\n",
    "\n",
    "ordinal_categories = [\n",
    "    ['2h', '1d'],   # expiration\n",
    "    ['High School or Less', 'Some College', 'Bachelors degree', 'Graduate degree (Masters or Doctorate)'], # education\n",
    "    ['never', 'less1', '1~3', '4+'],    # Bar\n",
    "    ['never', 'less1', '1~3', '4~8', 'gt8'],    # CoffeeHouse\n",
    "    ['less1', '1~3', '4~8', 'gt8'],    # CarryAway\n",
    "    ['less1', '1~3', '4~8', 'gt8'],    # RestaurantLessThan20\n",
    "    ['never', 'less1', '1~3', '4+'],     # Restaurant20To50\n",
    "    ['<21', '21-30', '31-40', '41-50', '51+'], # age_group\n",
    "    ['Under 12.5k', '12.5k-25k', '25k-37k', '37k-49k','50k-62k','62k-74k','75k-87k','87k-99k','100k+'] # income_group\n",
    "]\n",
    "\n",
    "ordinal_encoder = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc1a74",
   "metadata": {},
   "source": [
    "### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "2fd406da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fetures_ready_for_training():\n",
    "\n",
    "    data = pd.read_csv(\"data/in-vehicle-coupon-recommendation.csv\")\n",
    "    data = data.drop(['car','direction_opp'], axis=1)\n",
    "    data['age_numeric'] = data['age'].apply(convert_age_numeric)\n",
    "    data['age_group'] = data['age'].apply(convert_age_categorical)\n",
    "    data['income_numeric'] = data['income'].apply(convert_income_numeric)\n",
    "    data['income_group'] = data['income'].apply(convert_income_categorical)\n",
    "    data = data.drop(['age', 'income'], axis=1)\n",
    "\n",
    "    seed = 7\n",
    "    X = data.drop('Y', axis=1)\n",
    "    y = data['Y']\n",
    "\n",
    "    # Split for final training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "    # transformers\n",
    "    custom_cyclical_encoder = FunctionTransformer(custom_cyclical_encoder_func)\n",
    "    custom_balancing_function = FunctionTransformer(custom_balancing_func)\n",
    "    box_cox_transformer = PowerTransformer(method='box-cox', standardize=True)\n",
    "    scaler = StandardScaler()\n",
    "    one_hot_encoder = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    ordinal_encoder = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # columns\n",
    "    skewed_features = ['temperature', 'age_numeric']\n",
    "    numeric_features_to_scale = ['income_numeric']\n",
    "    nominal_cat_features = ['destination', 'passanger', 'weather', 'coupon', 'gender', 'maritalStatus', 'occupation']\n",
    "    ordinal_cat_features = ['expiration', 'education', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50', 'age_group', 'income_group']\n",
    "\n",
    "\n",
    "    # processors\n",
    "    column_preprocessor = ColumnTransformer(transformers = [\n",
    "        ('box_cox_transform', box_cox_transformer, skewed_features),\n",
    "        ('num_scale', scaler, numeric_features_to_scale),\n",
    "        ('nom', one_hot_encoder, nominal_cat_features),\n",
    "        ('ord', ordinal_encoder, ordinal_cat_features)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    preprocessor = Pipeline(steps=[\n",
    "        ('cyclical', custom_cyclical_encoder),\n",
    "        ('balancing', custom_balancing_function),\n",
    "        ('column_transforms', column_preprocessor)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    preprocessor.fit(X_train)\n",
    "    column_names = preprocessor.named_steps['column_transforms'].get_feature_names_out()\n",
    "    X__train_processed = preprocessor.transform(X_train)\n",
    "    X__test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    X_train_transformed = pd.DataFrame(X__train_processed, columns=column_names)\n",
    "    X_test_transformed = pd.DataFrame(X__test_processed, columns=column_names)\n",
    "\n",
    "    \n",
    "    return X_train_transformed, X_test_transformed, y_train, y_test, column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780072e",
   "metadata": {},
   "source": [
    "# The models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da635a",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78c08f",
   "metadata": {},
   "source": [
    "First model that we are going to try is Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eefd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Load and transform the data\n",
    "X_train_transformed, X_test_transformed, y_train, y_test, column_names = get_fetures_ready_for_training()\n",
    "\n",
    "\n",
    "# Init Logistic Regression model\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l2',           # regularization, shrinks coefficients evenly without eliminating some features entirely\n",
    "    solver='liblinear',     # good for small to medium datasets (which ours is)\n",
    "    random_state=42,        # common choise, makes sure our results are reproducable across different runs\n",
    "    max_iter=1000           # default is 100, increased to make sure the model has enough room to train, especially for our 12k records\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = log_reg.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nLogistic Regression Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best parameters for the model with grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42),\n",
    "                    param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train_transformed, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to make the performance of the model better by generating polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_transformed)\n",
    "X_test_poly = poly.transform(X_test_transformed)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(\n",
    "    C=0.1,                  # controls the strength of regularisation, best suggested option by grid search\n",
    "    penalty='l2',\n",
    "    solver='liblinear',\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = log_reg.predict(X_test_poly)\n",
    "\n",
    "# Validation Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nLogistic Regression Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38f1d1",
   "metadata": {},
   "source": [
    "Both polynomial parameters and the suggested C=0.1 parameter increase the model performance. So we will keep this version as the final one for Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aab9cc",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6dc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Load and transform the data\n",
    "X_train_transformed, X_test_transformed, y_train, y_test, column_names = get_fetures_ready_for_training()\n",
    "\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = rf_model.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "# Validation Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nRandom Forest Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 300, 500],        # number of trees\n",
    "    'max_depth': [5, 10, 15, 20],           # max depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],        # min number of samples required to split the node \n",
    "    'min_samples_leaf': [1, 2, 5],          # min number of samples at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],       # number of features to consider when looking for the best split\n",
    "    'class_weight': ['balanced']            # adjusts the weights the way that the classes are balanced\n",
    "}\n",
    "\n",
    "# Init the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Init RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # number of random combinations to try\n",
    "    scoring='f1',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Best parameters and best CV score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best CV Score:\", random_search.best_score_)\n",
    "\n",
    "# Best estimator\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_rf.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "# Validation Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nRandom Forest Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be884ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model with the best suggested parameters\n",
    "\n",
    "best_rf_tuned = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='log2',\n",
    "    max_depth=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "best_rf_tuned.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_rf_tuned.predict(X_test_transformed)\n",
    "\n",
    "# Validation Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nRandom Forest Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13cb199",
   "metadata": {},
   "source": [
    "We also tried to 1. remove the least imortant features, 2. select k top features with SelectKBest, 3. remove features with RFE ( recursive feature elimination ). But none of these imrpoved the model performance. So this last one will be left as final choise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d5918",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Load and transform the data\n",
    "X_train_transformed, X_test_transformed, y_train, y_test, column_names = get_fetures_ready_for_training()\n",
    "\n",
    "\n",
    "# Init the FNN model\n",
    "model_fnn = Sequential([\n",
    "    Input(shape=(X_train_transformed.shape[1],)),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# Set parameters, compile\n",
    "model_fnn.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history_FNN = model_fnn.fit(\n",
    "    X_train_transformed,\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    validation_data=(X_test_transformed, y_test)\n",
    ")\n",
    "\n",
    "\n",
    "# Predict on test set (get probabilities)\n",
    "y_pred_probs = model_fnn.predict(X_test_transformed)\n",
    "\n",
    "# Convert the probabilities to labels (0 or 1)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "# Validation Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nFNN Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971486e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "\n",
    "\n",
    "# Load and transform the data\n",
    "X_train_transformed, X_test_transformed, y_train, y_test, column_names = get_fetures_ready_for_training()\n",
    "\n",
    "\n",
    "# Build the model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_transformed.shape[1],)))\n",
    "\n",
    "    for i in range(hp.Int(\"num_hidden_layers\", 1, 5)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "            activation=hp.Choice(\"activation\", ['relu', 'tanh', 'elu']),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                hp.Float(f\"l2_rate_{i}\", 1e-5, 1e-2, sampling='log')\n",
    "            )\n",
    "        ))\n",
    "        model.add(Dropout(hp.Float(f\"dropout_{i}\", 0.0, 0.6, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    learning_rate = hp.Float(\"learning_rate\", 1e-5, 1e-2, sampling=\"log\")\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == \"rmsprop\":\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory=\"fnn_tuning\",\n",
    "    project_name=\"vehicle_coupon\"\n",
    ")\n",
    "\n",
    "# EarlyStopping, to stop if no improvement found for 5 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Start the search\n",
    "tuner.search(\n",
    "    X_train_transformed, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test_transformed, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe755fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# # Print all hyperparameter values\n",
    "# for param, value in best_hps.values.items():\n",
    "#     print(f\"{param}: {value}\")\n",
    "\n",
    "\n",
    "# Build model with best hyperparameters\n",
    "best_model_fnn = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train it\n",
    "history_fnn_best = best_model_fnn.fit(X_train_transformed, y_train, validation_data=(X_test_transformed, y_test), epochs=150)\n",
    "\n",
    "\n",
    "# Predict on test data (get probabilities)\n",
    "y_pred_probs = best_model.predict(X_test_transformed)\n",
    "\n",
    "# Convert the probabilities to labels (0 or 1)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "# Validation Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nFNN Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
